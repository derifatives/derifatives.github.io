  ---
layout: post
title: Welcome!
---

["Neural Scene De-rendering"](http://nsd.csail.mit.edu/), Wu, Tenenbaum, Kohli. CVPR 2017.

The authors develop a new interpretable scene representation. It's an encoder-decoder architecture, but the decoder is a deterministic non-differentiable renderer. The encoder has two pieces: a proposal generator and an object interpreter. The proposal generator uses [MNC](https://arxiv.org/abs/1512.04412), a segmentation method which predicts masks of candidate objects. The proposal generator computes 100 proposals per image. The object interpreter predicts, for each proposal, whether there is an object in the segment.

The internal representation is "Scene XML", which expicitly represents object category, size, color, position and pose. The objects are stereotyped shapes. Because the objects have depth, occlusion is possible, so Scene XML's with small edit distances can generate very different images. During training, the authors want to minimize both Scene XML distance and render distance. The renderer is non-differentiable, so the authors use a stochastic layer at the end of the encoder and REINFORCE. They can also tune just the render loss without scene XML labels, although this only works in a mixed curriculum-learning setting.

The authors also consider "analysis-by-synthesis refinement", where they treat the position as an initialization for a Gibbs sampler, run updates, and see whether it renders better. (I think this is basically just random search around the initial proposal in representation space.)

The authors use two experimental settings, the [Abstract Scene Dataset](https://vision.ece.vt.edu/clipart/) and a new Minecraft dataset with 12 different objects. The methods work better than some CNN or CNN+LSTM baselines, although maybe not quite as convincingly better as I might have hoped. Numerically the analysis-by-synthesis refinement helps, but in the examples they show I don't see any differences. Interestingly, on the Minecraft dataset, the reconstruction has 4x lower error than the CNN+LSTM baseline (5.05 vs 20.22), but humans only prefer the NSD images at < 60%. The authors mention that "margins are smaller on the Minecraft dataset because all algorithms perform better", but the tables don't fully support this, and I don't have any intuition for why the Minecraft task is easier. 

Code for the model and experiments don't seem to be available.

#mlpapers
